## Three customer churn models

```{r prelims, results='asis', echo=FALSE, message=FALSE} 
opts_chunk$set(cache=FALSE, echo=FALSE, comment="", fig.cap="")
```

```{r plot.options, echo=FALSE, message=FALSE}
require(grid)
require(gridExtra, quiet=TRUE)
require(ggplot2, quiet=TRUE)
theme_set(theme_gray(base_size = 18) + theme(axis.text.x = element_text(angle = 90, hjust = 1), plot.margin = unit(c(1, 2, 0.5, 0.5),"cm"), legend.position='none'))
read_chunk('churn_model.R')
```

```{r prep, message=FALSE}
```

I'm trying to replicate in R some models originally written in Python by Eric Chiang of $\hat{y}hat$ and [published on the company blog not too long ago](http://blog.yhathq.com/posts/predicting-customer-churn-with-sklearn.html).

We have `nrow(X)` cell phone customers with some usage data, some churned, some not, and we'll try to guess the best we can who churns and who doesn't. We will be using a support vector classifier (SVM), a random forest (rF), and a k nearest neighbor classifier (KNN), then we will compare their performance.

The timing for this blog post is perfect. It is of interest for me at work right now, and I also have the secondary goal of getting myself some GitHub practice, because I am scheduled to give my coworkers an introduction to our GitHub enterprise account soon. If I make it look easy, they might take to it.

So I cloned [Eric's code](https://github.com/EricChiang/churn) and got to work. My approximation of how his three models might be done in R is shown below. I define three functions for the job: `modelBakeOff()`, `groupThese()` and `summarizeThese()`. The last one does the work that Eric relegated to the `churn_measurements.py` script. You will want to see his comments and citations.


```{r runandcompare, echo=TRUE}
```

First, Eric shows a comparison of overall accuracy rates, then some confusion matrices in colorful graph format. Mine are below (with confusion matrices in plain text, good enough for right now):

```{r diagnostics}
```

My numbers are close enough to his so far. Next, some pictures:

```{r pictures}
```

```{r pics}
psvm + theme_gray(base_size = 12)
prf + theme_gray(base_size = 12)
```

The calibration and discrimination scores matter because they measure how close overall your bubbles are to your red lines. Obviously, the closer the better and bubble sizes matter. The classification and discrimination scores are essentially average squared distances weighted by bubble size.

Lower calibration is better, and higher discrimination is better. There's more detail in the original post. For now, here are mine:

```{r cdscores}
```

You will notice that my KNN scores are both entirely out of whack. Clearly, my attempt to replicate the defaults of `KNeighborsClassifier` in `sklearn.neighbors` failed miserably. But I did pretty well with the support vector classifier and the random forest, so I won't feel too bad. 

I concur with Eric that SVM beats rF on calibration, and rF beats SVM on discrimination, so the jury's still out. Just by eyeballing the two pictures above, my vote would go to SVM. I don't like that S pattern in rF. However, a comparison of the confusion matrices would endorse rF clearly. Both numbers on the main diagonal are larger in rF than they are in SVM.

My code is on [GitHub](https://github.com/ghuiber/churn/tree/Rversion). Fork it and let me know if you figure out how I managed to be so off on KNN. Here's my session info:

```{r info}
sessionInfo()
```
